<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DiffEmotionVC: A Dual-Granularity Disentangled Diffusion Framework for Any-to-Any Emotional Voice Conversion</title>
    <style>
        /* 基础样式 */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f7fb;
        }

        .container {
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        /* 统一的节样式 */
        .section {
            margin-bottom: 40px;
        }

        .section h2 {
            font-size: 2em;
            color: #333;
            margin-bottom: 10px;
        }

        .section p {
            font-size: 1.2em;
            color: #666;
            margin-bottom: 20px;
        }

        /* 摘要样式 */
        .abstract {
            text-align: left;
            padding: 20px;
            background: #f8f9ff;
            border-radius: 10px;
        }

        /* 模型图片样式 */
        .model-image {
            text-align: center;
            margin: 40px 0;
        }

        .model-image img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .model-image .caption {
            font-size: 1.2em;
            color: #666;
            margin-top: 10px;
        }

        /* 表格样式 */
        .demo-table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 40px;
        }

        .demo-table th, .demo-table td {
            border: 1px solid #ddd;
            padding: 15px;
            text-align: center;
            vertical-align: middle; /* 使按钮垂直居中 */
        }

        .demo-table th {
            background-color: #f8f9ff;
            font-weight: bold;
        }

        /* 按钮样式 */
        .button-container {
            display: flex;
            justify-content: center;
            gap: 10px;
        }

        .action-btn {
            background: #4a90e2;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s ease;
            min-width: 80px; /* 保证按钮有足够的宽度 */
        }

        .action-btn:hover {
            background: #357abd;
            transform: scale(1.05);
        }

        /* 音频文件样式 */
        audio {
            display: none; /* 隐藏音频元素 */
        }
        .show-audio{
            width: 70px;
            display: block;
        }
        .music-outer {
            display: flex;
            align-items: center;
            justify-content: center;
            flex-direction: column;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- 论文介绍 -->
        <div class="section">
            <h2>DiffEmotionVC: A Dual-Granularity Disentangled Diffusion Framework for Any-to-Any Emotional Voice Conversion</h2>
            <div class="abstract">
                <h3>Abstract</h3>
                <p>
                    Emotional Voice Conversion (EVC) plays a vital role in improving human-computer interaction but faces challenges due to the complexity of emotion features, which are entangled with speaker and content characteristics. To overcome these challenges, we propose DiffEmotionVC, a diffusion-based framework for any-to-any EVC. Our approach integrates a dual-granularity emotion encoder that captures both utterance-level emotional context and frame-level acoustic details. It also employs an orthogonality-constrained condition encoder that disentangles emotion features through gated cross-attention while preserving feature independence with an orthogonal loss. Additionally, multi-objective diffusion training enhances both reconstruction fidelity and emotion discriminability via contrastive learning. Experimental results show a UTMOS score of 4.04 and 80% emotion recognition accuracy, demonstrating the framework's effectiveness in speech quality and optimizing emotional expression.
                </p>
            </div>
        </div>

        <!-- 模型框架图片 -->
        <div class="section">
            <h2>DiffEmotionVC Model Architecture</h2>
            <div class="model-image">
                <img src="38.png" alt="DiffEmotionVC Model Architecture">
                <div class="caption">
                    <p><strong>Figure 1:</strong> The overview of the DiffEmotionVC framework includes three feature extractors and encoders for speaker, emotion, and content. It also incorporates a condition encoder that fuses these features using a cross-attention mechanism, maintains feature independence through orthogonal constraints, and utilizes a diffusion module to generate high-quality speech, where (a) represents the speaker encoder, and (b) represents the emotion encoder.</p>
                </div>
            </div>
        </div>

        <!-- 音频demo展示 -->
        <div class="section">
            <h2>Experimental Results on Emotional Voice Conversion</h2>

            <!-- Neutral to Angry -->
            <h3>Neutral to Angry</h3>
            <table class="demo-table">
                <tr>
                    <th>Neutral</th>
                    <th>CycleGAN-EVC</th>
                    <th>StarGAN-EVC</th>
                    <th>Seq2Seq-EVC</th>
                    <th>Emovox</th>
                    <th>Prosody2Vec</th>
                    <th>Ours</th>
                    <th>Target</th>
                    <th>Reference</th>
                </tr>
                <tr>
                    <td>
                        <div class="music-outer">
                            a1<audio class="neu-ang-1 show-audio" controlslist="noplaybackrate" controls src="./Source_n2a_1.wav"></audio>
                        </div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a2<audio class="neu-ang-1-CycleGAN-EVC show-audio" controlslist="noplaybackrate" controls src="./CycleGAN_EVC_n2a_1.wav"></audio>
                        </div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a3<audio class="neu-ang-1-stargan show-audio" controlslist="noplaybackrate" controls src="./StarGAN_EVC_n2a_1.wav"></audio>
                        </div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a4<audio class="neu-ang-1-seq2seq show-audio" controlslist="noplaybackrate" controls src="./Seq2Seq_EVC_n2a_1.wav"></audio></div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a5<audio class="neu-ang-1-emovox show-audio" controlslist="noplaybackrate" controls src="./Emovox_n2a_1.wav"></audio></div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a6<audio class="neu-ang-1-prosody2vec show-audio" controlslist="noplaybackrate" controls src="./Prosody2Vec_n2a_1.wav"></audio></div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a7<audio class="neu-ang-1-ours show-audio" controlslist="noplaybackrate" controls src="./Ours_n2a_1.wav"></audio></div>
                    </td>
                    <td>
                        <div class="music-outer">
                            a8<audio class="neu-ang-1-target show-audio" controlslist="noplaybackrate" controls src="./0013_000362.wav"></audio></div>
                    </td>
                    <td rowspan="3">
                        <div class="music-outer">
                            a9<audio class="neu-ang-1-reference show-audio" controlslist="noplaybackrate" controls src="./0013_000643.wav"></audio></div>
                    </td>
                </tr>
                <!-- Add more audio rows as needed -->
            </table>
        </div>

        <!-- 其他内容 -->
        <!-- Neutral to Happy -->
        <!-- Neutral to Sad -->
        <!-- The impact of different content extraction methods on emotion features -->
        <!-- Ablation Studies -->
    </div>
</body>
</html>
